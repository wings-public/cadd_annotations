"""Note that we are declaring many files temporary here that should be located in a temporary folder to begin with"""


# container with conda environments
#containerized: "docker://visze/cadd-scripts-v1_6:0.1.0"

# need psutil to get env
import psutil
# Min version of snakemake
from snakemake.utils import min_version

#min_version("8.25.2")

###################################
## RULE SPECFIC THREADING LIMITS ##
###################################
if int(config.get("mem_gb",0)) == 0:
    system_memory = psutil.virtual_memory().total / (1024 ** 3)
else:
    system_memory = int(config["mem_gb"])

###############
## CPU TASKS ##
###############
# then assign other resources
config['vep_cpu_load'] = 100 if system_memory < 4 else max(1,int(100/(max(1,int(0.9*system_memory / 4 )))))  # up to 4Gb/ram
config['vep_cpu_threads'] = max(1, int(workflow.cores / (100/config['vep_cpu_load'])))
#config['impute_cpu_load'] = 100 if system_memory < 4 else int(100/int(0.9*system_memory / 4 ))  # up to 4Gb/ram
config['regseq_cpu_load'] = 100 if system_memory < 2 else int(100/int(0.9*system_memory / 2 ))   # up to 2Gb of ram
config['anno_cpu_load'] = 1 # disk IO intensive
config['impute_cpu_load'] = 1
config['prescore_cpu_load'] = 6  # disk IO intensive (~max 16) 
config['score_cpu_load'] = 1

print("Threading Overview")
print("##################")
print("Assigned cores: {}".format(workflow.cores))
print("Available system memory: {}GB".format(int(system_memory)))

# print threading limits
print("Task Parallelization: ")
print("  PreScore : {}x".format(min(workflow.cores,int(100/config['prescore_cpu_load']))))
print("  VEP : {}x with {} threads each".format(min(workflow.cores,int(100/config['vep_cpu_load'])), config['vep_cpu_threads']))
#print("  RegSeq : {}x".format(min(workflow.cores,int(100/config['regseq_cpu_load']))))
#print("  MMsplice : {}x with {} threads each (memory constraints)".format(min(workflow.cores,int(100/config['mms_cpu_load'])),config['mms_cpu_threads']))
print("  Annotate : {}x".format(min(workflow.cores,int(100/config['anno_cpu_load']))))
print("  Impute : {}x".format(min(workflow.cores,int(100/config['impute_cpu_load']))))

# flush output before starting workflow
print("Starting workflow",flush=True)

## allowed scattering 
scattergather:
    split=workflow.cores,

rule decompress:
    input:
        "{file}.vcf.gz",
    output:
        temp("{file}.vcf"),
    shell:
        """
        zcat {input} > {output}
        """


rule prepare:
    input:
        temp("{file}.vcf"),
    output:
        #temp("{file}.prepared.vcf"),
        splits=scatter.split("{{file}}_splits/chunk_{scatteritem}.prepared.vcf"),
    conda:
        "envs/environment_minimal.yml"
    params:
        threads=workflow.cores,
    shell:
        """
        mkdir -p {wildcards.file}_splits/
        cat {input} \
        | python $CADD/src/scripts/VCF2vepVCF.py \
        | sort -k1,1 -k2,2n -k4,4 -k5,5 \
        | uniq > {wildcards.file}_splits/full.vcf

        LC=$(wc -l {wildcards.file}_splits/full.vcf | cut -f1 -d' ')
        ## if zero, stop the workflow here (as success)
        if [[ "$LC" -eq 0 ]]; then
            echo "No variants to process. Exiting."
            # put expected files in place for this step (generated by split and the for loop below)
            touch {wildcards.file}_splits/chunk_1-of-{params.threads}.prepared.vcf
            exit 0
        fi

        LC=$(((LC / {params.threads})+1))

        split -l $LC --numeric-suffixes=1 --additional-suffix="-of-{params.threads}.prepared.vcf" {wildcards.file}_splits/full.vcf {wildcards.file}_splits/chunk_
        rm -f {wildcards.file}_splits/full.vcf

        # strip padding zeros in the file names 
        for f in {wildcards.file}_splits/chunk_*.prepared.vcf
        do
            target=$(echo "$f" | sed -E 's/(chunk_)0*([1-9][0-9]*)(-of-{params.threads}\\.prepared\\.vcf)/\\1\\2\\3/')
            if [ "$f" != "$target" ]; then
                mv -n "$f" "$target"
            fi

        done
        """


rule prescore:
    input:
        vcf="{file}_splits/chunk_{chunk}.prepared.vcf",
        #"{file}.prepared.vcf",
    output:
        novel="{file}_splits/chunk_{chunk}.novel.vcf",
        prescored="{file}_splits/chunk_{chunk}.pre.tsv",
    conda:
        "envs/environment_minimal.yml"
    resources:
        # < 1GB of memory
        cpu_load=int(config['prescore_cpu_load']),
    shell:
        """
        # Prescoring
        echo '## Prescored variant file' > {output.prescored};
       if [ -d $CADD/{config[PrescoredFolder]} ]
        then
            for PRESCORED in $(ls $CADD/{config[PrescoredFolder]}/*.tsv.gz)
            do
                cat {input.vcf} \
                | python $CADD/src/scripts/extract_scored.py --header \
                    -p $PRESCORED --found_out={output.prescored}.tmp \
                > {input.vcf}.tmp;
                cat {output.prescored}.tmp >> {output.prescored}
                mv {input.vcf}.tmp {input.vcf};
            done;
            rm {output.prescored}.tmp
        fi
        mv {input.vcf} {output.novel}
        """


rule annotation:
    input:
        #"{file}.novel.vcf",
        vcf="{file}_splits/chunk_{chunk}.novel.vcf"
    output:
        "{file}_splits/chunk_{chunk}.anno.tsv.gz",
        #"{file}_splits/chunk_{chunk}.vep.vcf.gz",
    conda:
        "envs/environment_minimal.yml"
    params:
        threads=config['vep_cpu_threads'],
    resources:
        cpu_load=int(config['vep_cpu_load']),
    threads:
        config['vep_cpu_threads'],

    shell:
        """
        cat {input.vcf} \
        | vep --quiet --cache --offline --dir $CADD/{config[VEPpath]} \
            --buffer 1000 --no_stats --species homo_sapiens \
           --db_version={config[EnsemblDB]} --assembly {config[GenomeBuild]} \
            --format vcf --regulatory --sift b --polyphen b --per_gene --ccds --domains \
            --numbers --canonical --total_length --vcf --force_overwrite --output_file STDOUT \
            --fork {params.threads} \
        | python $CADD/src/scripts/annotateVEPvcf.py \
            -c $CADD/{config[ReferenceConfig]} \
        | gzip -c > {output}
        """


rule imputation:
    input:
        "{file}_splits/chunk_{chunk}.anno.tsv.gz",
    output:
        "{file}_splits/chunk_{chunk}.csv.gz",
    conda:
        "envs/environment_minimal.yml"
    resources:
        cpu_load=int(config['impute_cpu_load']),
    shell:
        """
        zcat {input} \
        | python $CADD/src/scripts/trackTransformation.py -b \
            -c $CADD/{config[ImputeConfig]} -o {output} --noheader;
        """


rule score:
    input:
        impute="{file}_splits/chunk_{chunk}.csv.gz",
        anno="{file}_splits/chunk_{chunk}.anno.tsv.gz",
    output:
        "{file}_splits/chunk_{chunk}.novel.tsv",
    conda:
        "envs/environment_minimal.yml"
    resources:
        cpu_load=config['score_cpu_load'],
    shell:
        """
        python $CADD/src/scripts/predictSKmodel.py \
            -i {input.impute} -m $CADD/{config[Model]} -a {input.anno} \
        | python $CADD/src/scripts/max_line_hierarchy.py --all \
       | python $CADD/src/scripts/appendPHREDscore.py \
            -t $CADD/{config[ConversionTable]} > {output};

        if [ "{config[Annotation]}" = 'False' ]
        then
            cat {output} | cut -f {config[Columns]} | uniq > {output}.tmp
            mv {output}.tmp {output}
        fi
        """


rule join:
    input:
        pre=gather.split("{{file}}_splits/chunk_{scatteritem}.pre.tsv"),
        novel=gather.split("{{file}}_splits/chunk_{scatteritem}.novel.tsv"),
    output:
        #"{file}.tsv.gz",
        "{file,.+(?<!\\.anno)}.tsv.gz"
    conda:
        "envs/environment_minimal.yml"
    shell:
        """
        (
        echo "{config[Header]}";
        cat {input.pre} {input.novel} | grep -v "^##" | grep "^#" | tail -n 1;
        #head -n 1 {input.novel};
        cat {input.pre} {input.novel} \
        | grep -v "^#" \
        | sort -k1,1 -k2,2n -k3,3 -k4,4 || true;
        ) | bgzip -c > {output};
        """
